# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lN0jPRhTBrIXCn4nN7J82YeH37eKTdJZ
"""

!pip install -q gradio

!pip install langchain-google-genai

!pip install python-dotenv

!pip install langchain-anthropic
!pip install langchain-mistralai
!pip install langchain-groq

from google.colab import files
files.upload()

from llm_content import model_functions

import gradio as gr

def generate_responses(user_prompt, *model_selections):
    selected_models = [model for model, selected in zip(model_functions.keys(), model_selections) if selected]
    responses = {}
    for model_name in selected_models:
        try:
            response = model_functions[model_name](user_prompt)
            responses[model_name] = response
        except Exception as e:
            responses[model_name] = f"Error generating response: {str(e)}"
    return responses

def vote(model_name):
    return f"Thanks for voting for {model_name}!"

def toggle_button(text, value=False):
    return gr.Button(
        "‚úì " + text if value else text,
        value=value,
        elem_classes=["checkbox-button"]
    )

with gr.Blocks(css="""
    .checkbox-button {border: 1px solid #ccc; padding: 5px 10px; border-radius: 5px; cursor: pointer;}
    .checkbox-button[value='true'] {background-color: #e6f3ff; border-color: #2196F3;}
""") as demo:
    gr.Markdown("# LLM ARENA ü§ñ")
    gr.Markdown("## Compare responses from different Large Language Models")

    with gr.Row():
        with gr.Column(scale=2):
            user_prompt = gr.Textbox(label="Enter your prompt:", lines=5)

            model_buttons = {}
            for model_name in model_functions.keys():
                model_buttons[model_name] = toggle_button(
                    model_name,
                    value=model_name in ["Google AI's Gemini", "Anthropic's Claude"]
                )

            generate_button = gr.Button("Generate Responses")

        with gr.Column(scale=3):
            output = gr.JSON(label="Responses")

    generate_button.click(
        generate_responses,
        inputs=[user_prompt] + list(model_buttons.values()),
        outputs=output
    )

    for model_name, button in model_buttons.items():
        button.click(
            lambda x: not x,
            inputs=[button],
            outputs=[button],
            show_progress=False,
        )

    with gr.Row():
        for model_name in model_functions.keys():
            vote_button = gr.Button(f"üëç Vote for {model_name}")
            vote_button.click(vote, inputs=[], outputs=gr.Textbox(label="Vote Result"))

    gr.Markdown("## LLM ARENA Information")
    gr.Markdown("""
    Welcome to LLM ARENA! This application allows you to compare responses from various large language models (LLMs) side-by-side. Here are some tips to get you started:

    ### How to Use:
    1. **Enter a prompt**: Type your question or statement in the text area.
    2. **Select models**: Click on the model checkboxes to toggle them on/off.
    3. **Generate responses**: Click the 'Generate Responses' button.
    4. **Vote**: Vote for the response you find most helpful.

    ### Included Models:
    - Google AI's Gemini
    - Anthropic's Claude
    - Mistral AI
    - LLaMA 3 8B
    - Mixtral 8x7B
    - LLaMA 3 70B
    - Gemma 7B

    ### Resources:
    - [Gradio Documentation](https://www.gradio.app/docs/)
    - [LangChain Documentation](https://langchain.readthedocs.io/en/latest/)
    - [Google AI's Gemini](https://ai.google/tools/)
    - [Anthropic's Claude](https://www.anthropic.com/)
    - [Mistral AI](https://mistralai.com/)
    - [Groq API](https://www.groq.com/)

    ---
    Created with ‚ù§ using Gradio
    """)

demo.launch()